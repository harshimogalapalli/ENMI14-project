# -*- coding: utf-8 -*-
"""ENIR Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d4ONNLg6VjPmIb5U9kMs1z46bgqRnWAV
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import plotly.express as px 
sns.set_style('darkgrid')
from datetime import datetime
from dateutil.tz import *
import re 
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, RandomizedSearchCV
from sklearn.dummy import DummyRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xgboost as xgb
from xgboost import DMatrix
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

data = pd.read_csv('SolarPrediction.csv')
data.info()
df=data.copy()

#Checking if there are missing values
data.isnull().sum()

data = data.sort_values(['UNIXTime'], ascending = [True])
data.head()

from pytz import timezone
import pytz
hawaii= timezone('Pacific/Honolulu')
data.index =  pd.to_datetime(data['UNIXTime'], unit='s')
data.index = data.index.tz_localize(pytz.utc).tz_convert(hawaii)
#Step 4
data['MonthOfYear'] = data.index.strftime('%m').astype(int)
data['DayOfYear'] = data.index.strftime('%j').astype(int)
data['WeekOfYear'] = data.index.strftime('%U').astype(int)
data['TimeOfDay(h)'] = data.index.hour
data['TimeOfDay(m)'] = data.index.hour*60 + data.index.minute
data['TimeOfDay(s)'] = data.index.hour*60*60 + data.index.minute*60 + data.index.second
data['TimeSunRise'] = pd.to_datetime(data['TimeSunRise'], format='%H:%M:%S')
data['TimeSunSet'] = pd.to_datetime(data['TimeSunSet'], format='%H:%M:%S')
data['DayLength(s)'] = data['TimeSunSet'].dt.hour*60*60 \
                           + data['TimeSunSet'].dt.minute*60 \
                           + data['TimeSunSet'].dt.second \
                           - data['TimeSunRise'].dt.hour*60*60 \
                           - data['TimeSunRise'].dt.minute*60 \
                           - data['TimeSunRise'].dt.second
#Step 5
data.drop(['Data','Time','TimeSunRise','TimeSunSet'], inplace=True, axis=1)
data.head()

#Step 6
data_one_day = data.loc['2016-09-29':'2016-09-30',:]

plt.figure(figsize = (12,3))
plt.plot(data_one_day.Radiation, 'o', markerfacecolor = 'b')


#Adjusting timezone of x-axis
plt.gca().xaxis_date('HST')

plt.legend()
plt.show()

#Step 7

#Analysing the ranges of the various features of the datset
data.describe()

#Step 8
fig, ax = plt.subplots(nrows =3, ncols =2 , figsize = (15,15))

sns.distplot(data.Radiation, ax = ax[0,0],color = 'r')
ax[0,0].set_xlabel('Solar radiation [W/m^2]', fontsize = 18)
ax[0,0].set_ylabel('', fontsize = 18)
sns.distplot(data.Temperature, ax = ax[0,1],color = 'r')
ax[0,1].set_xlabel('Temperature [F]', fontsize = 18)
ax[0,1].set_ylabel('', fontsize = 18)
sns.distplot(data.Pressure, ax = ax[1,0],color = 'r')
ax[1,0].set_xlabel('Pressure [Hg]', fontsize = 18)
ax[1,0].set_ylabel('', fontsize = 18)
sns.distplot(data.Humidity, ax = ax[1,1],color = 'r')
ax[1,1].set_xlabel('Humidity [%]', fontsize = 18)
ax[1,1].set_ylabel('', fontsize = 18)
sns.distplot(data.Speed, ax = ax[2,0],color = 'r')
ax[2,0].set_xlabel('Wind speed [miles/h]', fontsize = 18)
ax[2,0].set_ylabel('', fontsize = 18)
sns.distplot(data['WindDirection(Degrees)'], ax = ax[2,1],color = 'r')
ax[2,1].set_xlabel('Wind direction [Degrees]', fontsize = 18)
ax[2,1].set_ylabel('', fontsize = 18)

df['Month'] = df['Data'].apply(lambda y: re.search(r'^\d+', y).group(0))
df['Day'] = df['Data'].apply(lambda y: re.search(r'(?<=\/)\d+(?=\/)', y).group(0))
df['Year'] = df['Data'].apply(lambda y: re.search(r'(?<=\/)\d+(?=\s)', y).group(0))
df['Month'] = df['Data'].apply(lambda x: re.search(r'^\d+', x).group(0)).astype(np.int)
df['Day'] = df['Data'].apply(lambda y: re.search(r'(?<=\/)\d+(?=\/)', y).group(0)).astype(np.int)
df['Year'] = df['Data'].apply(lambda y: re.search(r'(?<=\/)\d+(?=\s)', y).group(0)).astype(np.int)

df

df=df.drop(['Data'],axis=1)

df

df['Hour'] = df['Time'].apply(lambda y: re.search(r'^\d+', y).group(0)).astype(np.int)
df['Minute'] = df['Time'].apply(lambda y: re.search(r'(?<=\:)\d+(?=\:)', y).group(0)).astype(np.int)
df['Second'] = df['Time'].apply(lambda y: re.search(r'\d+$', y).group(0)).astype(np.int)

del df['Time']

df['SunriseHour'] = df['TimeSunRise'].apply(lambda x: re.search(r'^\d+', x).group(0)).astype(np.int)
df['SunriseMinute'] = df['TimeSunRise'].apply(lambda x: re.search(r'(?<=:)\d+(?=:)', x).group(0)).astype(np.int)

df['SunsetHour'] = df['TimeSunSet'].apply(lambda x: re.search(r'^\d+', x).group(0)).astype(np.int)
df['SunsetMinute'] = df['TimeSunSet'].apply(lambda x: re.search(r'(?<=:)\d+(?=:)', x).group(0)).astype(np.int)

df = df.drop(['TimeSunRise', 'TimeSunSet'], axis=1)

df.dtypes

# Separating Dependent variable which is Radiation in this case from Independent variables
y = df['Radiation'].copy()
X = df.drop('Radiation', axis=1).copy()

scaler = StandardScaler()

Z = scaler.fit_transform(X)





pd.DataFrame(Z)

df.drop(['SunriseHour'],axis=1)

from sklearn.model_selection import KFold, RandomizedSearchCV
from sklearn.dummy import DummyRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import LinearRegression
kf = KFold(shuffle=True, random_state=19)

from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor, XGBRFRegressor
from catboost import CatBoostRegressor

trees = {
    'linear': LinearRegression(),
    'randomfor': RandomForestRegressor(random_state=19), 
    'gradientb': GradientBoostingRegressor(random_state=19), 
    'xgb': XGBRegressor(random_state=19), 
    'xgbrf': XGBRFRegressor(random_state=19), 
    'catboost': CatBoostRegressor(random_state=19, silent=True),
    'DecisionTr': DecisionTreeRegressor(random_state=19),
    'extratre': ExtraTreesRegressor(random_state=19),
    
}

!pip3 install catboost

scores = []
rmse=[]
mse=[]
mae=[]
for train_index, test_index in kf.split(X):
  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]

from sklearn import metrics 
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)
predictions = regression_model.predict(X_test)
scores.append(100*regression_model.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, regression_model.predict(X_test))))
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',regression_model.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

from sklearn import metrics 
Randomforest= RandomForestRegressor(random_state=7).fit(X_train, y_train)
scores.append(Randomforest.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, Randomforest.predict(X_test))))
predictions = Randomforest.predict(X_test)
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',Randomforest.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

from sklearn import metrics 
gb = GradientBoostingRegressor(random_state=19).fit(X_train, y_train)
scores.append(gb.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, gb.predict(X_test))))
predictions = gb.predict(X_test)
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',gb.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

from sklearn import metrics 
xgb = XGBRegressor(random_state=19).fit(X_train, y_train)
scores.append(xgb.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, xgb.predict(X_test))))
predictions = xgb.predict(X_test)
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',xgb.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

from sklearn import metrics 
xgbrfr = XGBRFRegressor(random_state=133).fit(X_train, y_train)
scores.append(xgbrfr.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, xgbrfr.predict(X_test))))
predictions = xgbrfr.predict(X_test)
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',xgbrfr.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

from sklearn import metrics 
cat= CatBoostRegressor(random_state=19, silent=True).fit(X_train, y_train)
scores.append(cat.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, cat.predict(X_test))))
predictions = cat.predict(X_test)
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',cat.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

dt = DecisionTreeRegressor(random_state=19).fit(X_train, y_train)
from sklearn import metrics 
scores.append(dt.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, dt.predict(X_test))))
predictions = dt.predict(X_test)
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',dt.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

et = ExtraTreesRegressor(random_state=19).fit(X_train, y_train)
from sklearn import metrics 
scores.append(et.score(X_test, y_test))
rmse.append(np.sqrt(mean_squared_error(y_test, et.predict(X_test))))
predictions = et.predict(X_test)
mse.append(metrics.mean_squared_error(y_test, predictions))
mae.append(metrics.mean_absolute_error(y_test, predictions))
print('R^2-Coefficient of Determination value',et.score(X_test, y_test))
print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) 

fig, ax = plt.subplots()
ax.scatter(y_test, predictions)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.set_title('R2: ' + str(r2_score(y_test, predictions)))
plt.show()

rmse